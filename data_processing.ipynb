{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# SMS Spam Detection - Data Processing\n",
       "\n",
       "This notebook covers the data exploration, cleaning, and preprocessing steps for the SMS Spam Detection project."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "import re\n",
       "import nltk\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.stem import WordNetLemmatizer\n",
       "\n",
       "# Download necessary NLTK data\n",
       "nltk.download('stopwords')\n",
       "nltk.download('wordnet')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Data Loading and Initial Exploration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the dataset\n",
       "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
       "\n",
       "# Drop unnecessary columns and rename the remaining ones\n",
       "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
       "df = df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
       "\n",
       "# Display basic information about the dataset\n",
       "print(df.info())\n",
       "print(\"\\nSample data:\")\n",
       "display(df.head())\n",
       "\n",
       "# Check for missing values\n",
       "print(\"\\nMissing values:\")\n",
       "print(df.isnull().sum())\n",
       "\n",
       "print(\"\\nColumn names:\")\n",
       "print(df.columns)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Data Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Display class distribution\n",
       "print(\"\\nClass distribution:\")\n",
       "print(df['label'].value_counts(normalize=True))\n",
       "\n",
       "# Visualize class distribution using Pandas plotting\n",
       "df['label'].value_counts().plot(kind='bar')\n",
       "plt.title('Class Distribution')\n",
       "plt.xlabel('Class')\n",
       "plt.ylabel('Count')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Feature Engineering"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Add a new feature: message length\n",
       "df['message_length'] = df['text'].apply(len)\n",
       "\n",
       "# Visualize message length distribution using Pandas plotting\n",
       "df.boxplot(column='message_length', by='label')\n",
       "plt.title('Message Length by Class')\n",
       "plt.suptitle('')  # This removes the automatic suptitle added by Pandas\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Time Series Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Add a timestamp to our data (assume messages are received in order)\n",
       "df['timestamp'] = pd.date_range(start='2023-01-01', periods=len(df), freq='H')\n",
       "\n",
       "# Group by date and class, count messages\n",
       "daily_counts = df.groupby([df['timestamp'].dt.date, 'label']).size().unstack(fill_value=0)\n",
       "\n",
       "# Plot time series of message counts\n",
       "daily_counts.plot(figsize=(12, 6))\n",
       "plt.title('Daily Message Counts')\n",
       "plt.xlabel('Date')\n",
       "plt.ylabel('Number of Messages')\n",
       "plt.legend(['Ham', 'Spam'])\n",
       "plt.show()\n",
       "\n",
       "# Calculate and plot rolling average\n",
       "rolling_avg = daily_counts.rolling(window=7).mean()\n",
       "rolling_avg.plot(figsize=(12, 6))\n",
       "plt.title('7-Day Rolling Average of Message Counts')\n",
       "plt.xlabel('Date')\n",
       "plt.ylabel('Average Number of Messages')\n",
       "plt.legend(['Ham', 'Spam'])\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Data Cleaning and Preprocessing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Text preprocessing function\n",
       "def preprocess_text(text):\n",
       "    # Convert to lowercase\n",
       "    text = text.lower()\n",
       "    # Remove special characters and digits\n",
       "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
       "    # Tokenize\n",
       "    tokens = text.split()\n",
       "    # Remove stopwords\n",
       "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
       "    # Lemmatize\n",
       "    lemmatizer = WordNetLemmatizer()\n",
       "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
       "    return ' '.join(tokens)\n",
       "\n",
       "# Apply preprocessing to the text column\n",
       "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
       "\n",
       "print(\"Sample cleaned data:\")\n",
       "display(df[['text', 'cleaned_text']].head())"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Export Cleaned Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Export cleaned data as CSV\n",
       "df.to_csv('cleaned_sms_data.csv', index=False)\n",
       "\n",
       "# Split the data into training and testing sets (75% train, 25% test)\n",
       "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label'], test_size=0.25, random_state=42)\n",
       "\n",
       "# Export train and test sets\n",
       "pd.DataFrame({'text': X_train, 'label': y_train}).to_csv('train_data.csv', index=False)\n",
       "pd.DataFrame({'text': X_test, 'label': y_test}).to_csv('test_data.csv', index=False)\n",
       "\n",
       "print(\"Data exported successfully!\")\n",
       "print(f\"Training set size: {len(X_train)} ({len(X_train) / len(df['cleaned_text']):.2%})\")\n",
       "print(f\"Test set size: {len(X_test)} ({len(X_test) / len(df['cleaned_text']):.2%})\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }